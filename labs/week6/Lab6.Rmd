---
title: "Lab 6"
author: "Jesse Wheeler"
date: "`r Sys.Date()`"
output: slidy_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Today's plan

1. Announcements 
1. Bootstrap   
1. HW / Project Questions 

## Announcments

- Project 2: Due today 
- Homework 2: Due Sunday 

## Boostrap: Formal Introduction

Let $X_1, X_2, \ldots, X_n$ be i.i.d. random variables from some unknown distribution $Q$. 
For convenience, we denote all observations by the vector $X_{1:n} = (X_1, \ldots, X_n)$. 

We don't need to make any assumptions about $Q$. For arbitrary $Q$, the *empirical distribution* $\widehat{Q}$ is a natural estimate of $Q$:
$$
\widehat{Q} = \frac{1}{n}\sum_{i = 1}^n \delta_{X_i},
$$
where $\delta_x$ represents a point mass at $x$. 

Note that $\widehat{Q}$ is a valid probability distribution; it is a discrete distribution that assigns mass $1/n$ to each data point $X_i$; for any set $A$, $\widehat{Q}(A)$ is the proportion of these values that lie in $A$: 
$$
\widehat{Q}(A) = \frac{1}{n}\left|\{i \leq n: X_i \in A\}\right|. 
$$
By the law of large numbers, $\widehat{Q} \rightarrow Q(A)$ in probability. 

Now suppose that we are interested in a parameter $\theta$ of this distribution: $\theta = \theta(Q)$ (for example, the mean, median, variance, etc). 
Let $\widehat{\theta} = \widehat{\theta}(X_{1:n})$ be an estimate of $\theta$. 

We are almost always interested in how good our estimate is.
One way to quantify this is to consider the distribution of $\widehat{\theta} - \theta$. 
This distribution contains information about our estimate, including the bias and variance. Because the error distribution depends on $Q$, which is unknown, we unfortunately can't say much about $\widehat{\theta} - \theta$.

This is where bootstrap comes into play: Because $\widehat{Q} \rightarrow Q$, it's reasonable to think that the error distribution may be similar to the error distribution using samples drawn from $\widehat{Q}$. 

Given the original data $X_{1:n}$, let $X_{1:n}^*$ denote a bootstrap sample. Then

$$
X_{1:n}^*|X_{1:n} \sim \widehat{Q}
$$
and $X_{1:n}^*$ can be drawn by taking samples from $X_{1:n}$ with replacement. We then can make estimates:
$$
\widehat{\theta}^* = \widehat{\theta}(X_{1:n}^*),
$$
and the distribution $\widehat{\theta}^* - \widehat{\theta}$ can be used to estimate $\widehat{\theta} - \theta$. 

## Bias Reduction 

The bootstrap technique is often used to estimate parameter uncertainty. 
It has various other uses, however, including *bias reduction*. 

Let $Q$ be the unknown, arbitrary distribution from which we get our data (parameter or non-parametric). Then define the *bias* $b$ as:
$$
b = b(Q) = E_Q[\widehat{\theta} - \theta]
$$
If the bias were known, then $\widetilde{\theta} = \widehat{\theta} - b$ would be an un-biased estimator for $\theta$. 
Note that the true bias depends only on the error distribution.
Therefore let's substitute the bootstrap estimate for the error distribution: 
$$
\widehat{b} = E[\widehat{\theta}^* - \widehat{\theta} | X_{1:n}]
$$
Now we can make a new estimator $\widetilde{\theta}^*$:

$$
\widetilde{\theta}^* = \widehat{\theta} - \widehat{b}
$$

Then $\widetilde{\theta}^*$ is generally less biased than $\widehat{\theta}$. 

In practice, $\widehat{b}$ can be well-approximated using the law of large numbers: 
$$
\widehat{b} \approx \frac{1}{B}\sum_{i = 1}^B [\widehat{\theta}_i^* - \widehat{\theta}],
$$
where $\widehat{\theta}_i^* = \widehat{\theta}(X^*_{1:n, i})$ is the estimate from the $i$th bootstrap sample $X^*_{1:n, i}$, and $B$ is the number of samples. 

### Example: Estimating $\theta = \mu^3$

Let $\mu = EX_i$, and consider estimating $\mu^3$. Because the sample mean is an un-biased estimator of $\mu$, a natural estimator of $\mu^3$ is $\bar{X}^3$. 

Let $\sigma^2 = Var(X_i)$, and $\gamma = E(X_i - \mu)^3$. Using independence and the properties of expectations, it is easy to show that

$$
E(\bar{X} - \mu)^2 = \frac{\sigma^2}{n}, \, \text{ and } E(\bar{X} - \mu)^3 = \frac{\gamma}{n^2},
$$
Therefore
\begin{align}
E\bar{X}^3 &= E(\bar{X} - \mu + \mu)  \\
&= E\left[\mu^3 + 3\mu^2(\bar{X} - \mu) + 3\mu(\bar{X} - \mu)^2 + (\bar{X} - \mu)^3\right] \\
&= \mu^3 + \frac{3\mu\sigma^2}{n} + \frac{\gamma}{n^2}.
\end{align}
Therefore taking differences tells use that the bias of our estimator $\widehat{\theta} = \bar{X}^3$ as bias: 
$$
b = b(Q) = \widehat{\theta} - \theta = \frac{3\mu\sigma^2}{n} + \frac{\gamma}{n^2} = O(1/n).
$$
Doing the same thing, but now substituting $\widehat{Q}$ for the distribution $Q$, so that
$$
\widehat{\sigma}^2 = E_{\widehat{Q}}(X_i - \bar{X})^2 = \frac{1}{n}\sum_{i = 1}^n (X_i - \bar{X})^2,
$$

and 

$$
\widehat{\gamma} = E_{\widehat{Q}}(X_i - \bar{X})^3 = \frac{1}{n}\sum_{i = 1}^n (X_i - \bar{X})^3
$$

Then similar calculations to those that we did before gives:
$$
\widehat{b} = \frac{3\bar{X}\widehat{\sigma}^2}{n} + \frac{\widehat{\gamma}}{n^2}. 
$$
Therefore our biased-reduced estimator is:
$$
\widetilde{\theta} = \widehat{\theta} - \widehat{b} = \bar{X}^3 - \frac{3\bar{X}\widehat{\sigma}^2}{n} + \frac{\widehat{\gamma}}{n^2}.
$$
To check the bias of our new estimator, we now need to consider $E[\widetilde{\theta}]$. 
This isn't too difficult, but it does involve many tedious calculations that we don't really want to spend time covering here. 
The result of the calculation is: 

$$
E[\widetilde{\theta}] = \mu^3 + \frac{3}{n^2}(\mu\sigma^2 - \gamma) + \frac{6\gamma}{n^3} - \frac{2\gamma}{n^4} = \mu^3 + O\left(\frac{1}{n^2}\right)
$$
And therefore the bias of the adjusted estimator is of order $1/n^2$, rather than $1/n$. 

## CLT and Boostrap Accuracy

Bootstrap confidence intervals are so useful because they do not require many assumptions. 
They also enable us to obtain confidence intervals when an analytic confidence intervals are intractable. 

The important question that is worth considering is how well a boot-strap confidence interval actually is.
Unfortunately it is difficult to make any statements about a general boot strap confidence interval.
Fortunately, there are a few cases were we can make theoretical comparisons to other types of confidence intervals. 

As before, assume $X_{1:n}$ are i.i.d. observations from any arbitrary distribution $Q$. 
Consider estimating $\theta = E[X_i]$, letting $\widehat{\theta} = \bar{X}_{n}$.
Furthermore, let $\sigma^2 = Var(X_i)$, and:
$$
\widehat{\sigma}^2 = \frac{1}{n}\sum_{i = 1}^n (X_i - \bar{X}_{n})^2,
$$
which is the variance of the empirical distribution $\widehat{Q}_n$.
Finally, let $\gamma = E(X_i - \theta)^3$. 

If
$$
Z_n = \frac{\sqrt{n}(\widehat{\theta} - \theta)^2}{\sigma},
$$
then by the CLT, $P(Z_n \leq x) = \Phi(x) + O\left(\frac{1}{\sqrt{n}}\right)$. 

One way to prove the CLT is by the use of a second degree Taylor series approximation. 
What happens if we approximate using an additional term of the Taylor polynomial?
This is known as the *Edgeworth Expansion*, and gives a better approximation of the CLT: 
$$
P(Z_n \leq x) = \Phi(x) - \frac{\gamma(x^2 - 1)}{6\sigma^3\sqrt{n}}\phi(x) + o\left(\frac{1}{\sqrt{n}}\right)
$$
Obviously this isn't as useful as the CLT because it requires us to know $\gamma$ and $\sigma^3$, which are generally unknwon. 
We can, however, use this to compare to the accuracy of the boot-strap estimator. 

Let 
$$
Z_n^* = \frac{\sqrt{n}(\widehat{\theta}^* - \widehat{\theta})}{\widehat{\sigma}_n}.
$$
Theorem 19.4 of Theoretical Statistics: Topics for a Core Course (Robert Keener) gives the following result: 
$$
P(Z^*_n \leq x|X_{1:n}) = \Phi(x) - \frac{\gamma(x^2 - 1)}{6\sigma^3\sqrt{n}}\phi(x) + o_p\left(\frac{1}{\sqrt{n}}\right)
$$

What does this mean? 
One implication is that the bootstrap estimate for the confidence interval of the estimate of the mean of a non-parametric distribution is an improvement over the CLT. 

For more results on the performance of bootstrap estimates, see Chapter 19 of Keener. 

## Parametric Bootsrap 

There are several different version of bootstrap confidence intervals. 
One version is call the *parametric bootstrap*. 
In this case, we assume the data are i.i.d. from a distribution $Q$ that is a member of a parametric family $\{Q_\lambda:\lambda \in \Lambda\}$, where $\Lambda$ is the set of possible parameter values. 

Because in this case we know (or assume) the family of distribution of $Q$, our bootstrap distribution $\widehat{Q} = Q_{\widehat{\lambda}}$, where $\widehat{\lambda}$ is the maximum likelihood estimator of $\lambda$.
Then, the approach is similar to before.
Let $X^*_{1:n}$ be a bootstrap sample, so that:
$$
X^*_{i}|X_{1:n} \sim \widehat{Q}
$$
and the error distribution for $\widehat{\theta}$ would be estimated as the conditional distribution for $\widehat{\theta}^* - \widehat{\theta}$ given $X_{1:n}$, with $\widehat{\theta}^*$ being the parameter estimate from a single bootstrap sample. 

For example, consider the interval estimation of the mean of a normal distribution.
Letting $\lambda = (\theta, \sigma)$, $\widehat{\lambda} = (\widehat{\mu}, \widehat{\sigma})$, with the standard estimates for $\sigma^2$ and $\mu$. 
We then obtain bootstrap estimates as:
$$
\widehat{\mu}^*|X_{1:n} \sim N(\bar{X}_n, \widehat{\sigma}^2/n),
$$
So the bootstrap confidence interval is: 
$$
\left(\bar{X}_n - Z_{\alpha/2}\frac{\widehat{\sigma}}{\sqrt{n}}, \bar{X}_n + Z_{\alpha/2}\frac{\widehat{\sigma}}{\sqrt{n}}\right)
$$

## HW / Project Questions

We will treat this time as an extra set of office hours. 