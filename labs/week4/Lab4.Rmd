---
title: "Week 4 Lab"
output:
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(tidyverse)
knitr::opts_chunk$set(echo = FALSE)
```

## Today's plan

1. Announcements 
1. Brief intro of `learnr`
1. Permutation Test
1. Experimental Design
1. HW / Project Questions 

## Announcments

1. Homework 1 due Friday 
1. If you haven't done so, do the peer review (Your peer review is considered finished when you have filled out the rubric and left a comment on the assignment summarizing your score).
1. Tomorrow there will be an intro to project 2. 

## `learnr`

- The `learnr` package is a fantastic tool to create interactive slides in `R` (these slides were created with `learnr`). 

- It's just as easy as `.Rmd` documents, but it allows you to run code interactively with the rendered slides! 

- Great tool to use for labs. 


### Example 

*Here's a simple exercise with an empty code chunk provided for entering the answer.*

Write the R code required to add two plus two:

```{r two-plus-two, exercise=TRUE}

```

### Adding Hints

*Here's an exercise where the chunk is pre-evaluated via the `exercise.eval` option (so the user can see the default output we'd like them to customize). We also add a "hint" to the correct solution via the chunk immediate below labeled `print-limit-hint`.*

Modify the following code to limit the number of rows printed to 5:

```{r print-limit, exercise=TRUE, exercise.eval=TRUE}
mtcars
```

```{r print-limit-hint}
head(mtcars)
```

## Permutation Tests 

Permutation tests rely on *Exchangability*. 
Suppose we have data from two groups, $
In a permutation test, the null hypothesis is: 

$$
H_0: P(X_1,X_2, \ldots, X_{n}, Y_1, Y_2, \dots, Y_{m}) = P(Z_{\sigma(1)}, \ldots, Z_{\sigma(n+m)})
$$
Where all $Z_i$s correspond to one of the $X_i$s or $Y_i$s, and $\sigma$ is any permutation. 

Here's an example, similar to what we did in class, but this time we want to compare the sample variance of to groups rather than the mean: 

```{r permute-variance, exercise=TRUE, exercise.eval=TRUE}
set.seed(123)

df <- data.frame(
  group = rep(1:2, each = 100),
  val = c(rnorm(100, mean = 0, sd = 1), rnorm(100, mean = 0, sd = 2))
)

data_stat <- df %>%
  group_by(group) %>%
  summarize(sample_var = var(val))

get_permuted_stat <- function(i) {
  df %>%
    mutate(group = sample(group)) %>%
    group_by(group) %>%
    summarize(sample_var = var(val))
}

permuted_res <- bind_rows(map(1:100, get_permuted_stat), .id = "permutation")
ggplot() + 
  geom_histogram(data = permuted_res, aes(sample_var), bins = 15, col = 'black') + 
  facet_wrap(~group) + 
  geom_vline(data = filter(data_stat, group == 1), aes(xintercept = filter(data_stat, group == 1) %>% pull(sample_var)), linetype = 'dashed') + 
   geom_vline(data = filter(data_stat, group == 2), aes(xintercept = filter(data_stat, group == 2) %>% pull(sample_var)), linetype = 'dashed') + 
  theme_bw()

```

### Surrogate data testing

Permutation tests are closely related to *Surrogate data testing*. 
This class of tests are sometimes called a "statistical proof by contradiction", however they aren't really a proof: we specify a null distribution, generate several surrogate data sets this null distribution, and then compare the observed data to the surrogate data using any statistic we want. 
If the statistic calculated on the data is very different than the statistic calculated on the surrogate data, we may reasonably conclude that the data generating mechanism is different. 
These tests are fairly popular in time-series modeling. 

Here's an example where we test whether or not a time-series model is generated by white noise (Gaussian). 

Our null hypothesis for the example below is that $X_t = \epsilon_t$, where $\epsilon_t \sim N(0, 1)$, but the data are actually generated from $X_t = 0.72 X_{t-1} + \epsilon_t$. 

```{r arima, exercise=TRUE, exercise.eval=TRUE}
set.seed(456)

X <- arima.sim(model = list(ar = c(0.72), ma = c(-.2)), n = 100)

wn_acf <- function(i) {
  Y <- arima.sim(n = 100, model = list())
  acf(Y, lag.max = 1, plot = FALSE)$acf[2]
}

df <- data.frame(
  type = c(rep('null', 1000), "data"),
  val = c(map_dbl(1:1000, wn_acf), acf(X, lag.max = 1, plot = FALSE)$acf[2])
)

ggplot() + 
  geom_histogram(data = filter(df, type == 'null'), aes(val), col = 'black',
                breaks = seq(-0.35, 0.4, 0.05)) + 
  geom_vline(xintercept = filter(df, type == 'data') %>% pull(val), col = 'red',
             linetype = 'dashed') + 
  theme_bw()

```

- What are advantages of this type of test? 
- What are the disadvantages? 

What happens if we pick a summary statistic that doesn't help us differentiate? 

```{r armaMeans, exercise=TRUE, exercise.setup = "arima"}
wn_mean <- function(i) {
  Y <- arima.sim(n = 100, model = list())
  mean(Y)
}

df2 <- data.frame(
  type = c(rep('null', 1000), "data"),
  val = c(map_dbl(1:1000, wn_mean), mean(X))
)

ggplot() + 
  geom_histogram(data = filter(df2, type == 'null'), aes(val), col = 'black',
                 breaks = seq(-0.35, 0.4, 0.05)) + 
  geom_vline(xintercept = filter(df2, type == 'data') %>% pull(val), col = 'red',
             linetype = 'dashed') + 
  theme_bw()
```


## Experimental Designs

## HW / Project Questions

We will treat this time as an extra set of office hours. 
